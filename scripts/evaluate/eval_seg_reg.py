#!/usr/bin/env python3
import os
import re
import json
import argparse
import subprocess
from datetime import datetime
from typing import Dict, Optional, Tuple, List

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import wandb
import matplotlib.pyplot as plt
from monai.metrics import SSIMMetric
from monai.transforms import Compose, LoadImageD, EnsureChannelFirstD, SpacingD, ResizeWithPadOrCropD, ScaleIntensityD


# -------------------------
# Constants / label map
# -------------------------
SYNTHSEG_CODEMAP = {
    0:  "background",
    2:  "left_cerebral_white_matter",
    3:  "left_cerebral_cortex",
    4:  "left_lateral_ventricle",
    5:  "left_inferior_lateral_ventricle",
    7:  "left_cerebellum_white_matter",
    8:  "left_cerebellum_cortex",
    10: "left_thalamus",
    11: "left_caudate",
    12: "left_putamen",
    13: "left_pallidum",
    14: "third_ventricle",
    15: "fourth_ventricle",
    16: "brain_stem",
    17: "left_hippocampus",
    18: "left_amygdala",
    24: "csf",
    26: "left_accumbens_area",
    28: "left_ventral_dc",
    41: "right_cerebral_white_matter",
    42: "right_cerebral_cortex",
    43: "right_lateral_ventricle",
    44: "right_inferior_lateral_ventricle",
    46: "right_cerebellum_white_matter",
    47: "right_cerebellum_cortex",
    49: "right_thalamus",
    50: "right_caudate",
    51: "right_putamen",
    52: "right_pallidum",
    53: "right_hippocampus",
    54: "right_amygdala",
    58: "right_accumbens_area",
    60: "right_ventral_dc"
}


# AE preprocessing for intensity metrics (T1->AE, etc.)
# If you trained with a specific AE grid, set these to match.
# If your gt_followup_path_AEspace is already saved from your predict script, these just help for T1->AE transform.
INPUT_SHAPE_AE = (120, 144, 120)
RESOLUTION_AE = 1.3

IMG_PIPE_AE = Compose([
    LoadImageD(keys=["img"], image_only=True),
    EnsureChannelFirstD(keys=["img"]),
    SpacingD(keys=["img"], pixdim=RESOLUTION_AE),
    ResizeWithPadOrCropD(keys=["img"], spatial_size=INPUT_SHAPE_AE, mode="minimum"),
    ScaleIntensityD(keys=["img"], minv=0.0, maxv=1.0),
])


# -------------------------
# Helpers
# -------------------------
def run_cmd(cmd: List[str], check=True):
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if check and p.returncode != 0:
        raise RuntimeError(
            "Command failed:\n"
            + " ".join(cmd)
            + "\n\nSTDOUT:\n"
            + p.stdout
            + "\n\nSTDERR:\n"
            + p.stderr
        )
    return p


def voxel_volume_mm3(path: str) -> float:
    img = nib.load(path)
    vs = nib.affines.voxel_sizes(img.affine)  # (sx,sy,sz) in mm
    return float(np.prod(vs))


def same_grid(a_path: str, b_path: str, rtol=0, atol=1e-3) -> bool:
    a = nib.load(a_path)
    b = nib.load(b_path)
    if a.shape != b.shape:
        return False
    return np.allclose(a.affine, b.affine, rtol=rtol, atol=atol)


def resample_to_ref_ants(moving: str, ref: str, out: str, interp: str) -> str:
    """
    Uses antsApplyTransforms with identity to resample `moving` onto `ref` grid.
    interp: "Linear" or "NearestNeighbor"
    """
    os.makedirs(os.path.dirname(out), exist_ok=True)
    cmd = [
        "antsApplyTransforms",
        "-d", "3",
        "-i", moving,
        "-r", ref,
        "-o", out,
        "-n", interp,
        "-t", "identity",
    ]
    run_cmd(cmd, check=True)
    return out


def load_nii_f32(path: str) -> np.ndarray:
    return nib.load(path).get_fdata().astype(np.float32)


def load_nii_i32(path: str) -> np.ndarray:
    return nib.load(path).get_fdata().astype(np.int32)


def minmax01(x: np.ndarray) -> np.ndarray:
    x = x.astype(np.float32)
    mn, mx = float(np.min(x)), float(np.max(x))
    return (x - mn) / (mx - mn + 1e-8)


def ssim_3d_monai(pred_path: str, true_path: str, device: str = "cpu") -> float:
    """
    SSIM over full 3D volume (single scalar).
    Assumes both volumes already aligned.
    """
    ssim_metric = SSIMMetric(spatial_dims=3, data_range=1.0).to(device)

    p = minmax01(load_nii_f32(pred_path))
    t = minmax01(load_nii_f32(true_path))

    # MONAI expects [B,C,D,H,W]
    p_t = torch.from_numpy(p)[None, None].to(device)
    t_t = torch.from_numpy(t)[None, None].to(device)

    with torch.no_grad():
        val = ssim_metric(p_t, t_t).item()
    return float(val)


def register_and_jacobian_syn(pred_path: str, true_path: str, out_dir: str, tag: str) -> Tuple[str, str]:
    """
    Register moving=pred to fixed=true using SyN and compute log-Jacobian of the warp.
    Output:
      warp_field: <tag>_1Warp.nii.gz
      jac_log:    <tag>_jacobian_log.nii.gz
    """
    os.makedirs(out_dir, exist_ok=True)
    prefix = os.path.join(out_dir, f"{tag}_")

    reg_cmd = [
        "antsRegistration",
        "--dimensionality", "3",
        "--float", "0",
        "--output", f"[{prefix},{prefix}warped.nii.gz]",
        "--interpolation", "Linear",
        "--winsorize-image-intensities", "[0.005,0.995]",
        "--use-histogram-matching", "0",
        "--initial-moving-transform", f"[{true_path},{pred_path},1]",
        "--transform", "SyN[0.1,3,0]",
        "--metric", f"CC[{true_path},{pred_path},1,4]",
        "--convergence", "[100x70x50x20,1e-6,10]",
        "--shrink-factors", "8x4x2x1",
        "--smoothing-sigmas", "3x2x1x0vox",
    ]
    run_cmd(reg_cmd, check=True)

    warp_field = f"{prefix}1Warp.nii.gz"
    jac_log = f"{prefix}jacobian_log.nii.gz"

    jac_cmd = [
        "CreateJacobianDeterminantImage", "3",
        warp_field,
        jac_log,
        "1",  # 1 => log-Jacobian
        "0"
    ]
    run_cmd(jac_cmd, check=True)
    return warp_field, jac_log


def displacement_stats_from_warp(warp_path: str, mask: Optional[np.ndarray] = None) -> Dict[str, float]:
    warp = nib.load(warp_path).get_fdata()
    # warp can be (X,Y,Z,3) or (3,X,Y,Z)
    if warp.ndim == 4 and warp.shape[-1] == 3:
        disp = np.linalg.norm(warp, axis=-1)
    elif warp.ndim == 4 and warp.shape[0] == 3:
        disp = np.linalg.norm(warp, axis=0)
    else:
        raise ValueError(f"Unexpected warp shape: {warp.shape}")

    if mask is not None:
        disp = disp[mask > 0]

    return {
        "reg_disp_mean": float(np.mean(disp)),
        "reg_disp_median": float(np.median(disp)),
        "reg_disp_p95": float(np.percentile(disp, 95.0)),
        "reg_disp_max": float(np.max(disp)),
    }


def jacobian_stats(jac_log_path: str, mask: Optional[np.ndarray] = None, thresh_abs_logjac: float = 0.1) -> Dict[str, float]:
    jac = nib.load(jac_log_path).get_fdata().astype(np.float32)
    if mask is not None:
        jac = jac[mask > 0]
    absj = np.abs(jac)
    return {
        "jac_mean_abs_logjac": float(absj.mean()),
        f"jac_frac_abslogjac_gt_{thresh_abs_logjac:.2f}": float((absj > thresh_abs_logjac).mean()),
        "jac_frac_negative": float((jac < 0).mean()),
        "jac_p95_abs_logjac": float(np.percentile(absj, 95.0)),
    }


def compute_dice_per_label(pred: np.ndarray, true: np.ndarray) -> Dict[int, float]:
    labels = np.unique(true)
    out = {}
    for lbl in labels:
        if lbl == 0:
            continue
        p = (pred == lbl)
        t = (true == lbl)
        denom = p.sum() + t.sum()
        if denom > 0:
            out[lbl] = float(2.0 * (p & t).sum() / denom)
    return out


def compute_abs_volumes(seg: np.ndarray, voxvol: float) -> Dict[str, float]:
    out = {}
    labels = np.unique(seg)
    for lbl in labels:
        if lbl == 0:
            continue
        name = SYNTHSEG_CODEMAP.get(int(lbl), f"label_{int(lbl)}")
        out[f"absvol_{name}"] = float((seg == lbl).sum() * voxvol)
    return out


def compute_rel_volumes(seg: np.ndarray) -> Dict[str, float]:
    out = {}
    labels = [lbl for lbl in np.unique(seg) if lbl != 0]
    if len(labels) == 0:
        return out
    total = float((seg != 0).sum())
    if total <= 0:
        return out
    for lbl in labels:
        name = SYNTHSEG_CODEMAP.get(int(lbl), f"label_{int(lbl)}")
        out[f"relvol_{name}"] = float((seg == lbl).sum() / total)
    return out


def synthseg_batch(images: List[str], out_segs: List[str], threads: int, use_cpu: bool, extra_flags: Optional[List[str]] = None):
    """
    Run SynthSeg once on a textfile list of inputs/outputs (recommended by SynthSeg docs).
    """
    assert len(images) == len(out_segs) and len(images) > 0
    os.makedirs(os.path.dirname(out_segs[0]), exist_ok=True)

    # write temp lists
    in_list = os.path.join(os.path.dirname(out_segs[0]), "synthseg_inputs.txt")
    out_list = os.path.join(os.path.dirname(out_segs[0]), "synthseg_outputs.txt")
    with open(in_list, "w") as f:
        f.write("\n".join(images) + "\n")
    with open(out_list, "w") as f:
        f.write("\n".join(out_segs) + "\n")

    cmd = ["mri_synthseg", "--i", in_list, "--o", out_list, "--threads", str(int(threads))]
    if use_cpu:
        cmd.append("--cpu")
    if extra_flags:
        cmd.extend(extra_flags)

    try:
        run_cmd(cmd, check=True)
    finally:
        for p in (in_list, out_list):
            try:
                os.remove(p)
            except FileNotFoundError:
                pass


# -------------------------
# Main
# -------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--manifest_csv", required=True, help="manifest produced by prediction step")
    ap.add_argument("--out_dir", required=True, help="output directory for eval artifacts + CSV")
    ap.add_argument("--fold", type=str, default=None, help="Optional: filter manifest rows by fold (e.g. 1)")
    ap.add_argument("--fold_col", type=str, default="fold")
    ap.add_argument("--max_cases", type=int, default=None, help="Debug: limit number of cases")
    ap.add_argument("--wandb_project", type=str, default="decoder_eval_manifest")
    ap.add_argument("--wandb_mode", type=str, default="offline", choices=["offline", "online", "disabled"])
    ap.add_argument("--device", type=str, default="cpu", choices=["cpu", "cuda"])

    # SynthSeg
    ap.add_argument("--run_synthseg", action="store_true", help="If set, run SynthSeg on all predicted followups")
    ap.add_argument("--synthseg_threads", type=int, default=int(os.getenv("SLURM_CPUS_PER_TASK", "8")))
    ap.add_argument("--synthseg_cpu", action="store_true", help="Force SynthSeg CPU mode")
    ap.add_argument("--synthseg_fast", action="store_true", help="Use --fast")
    ap.add_argument("--synthseg_robust", action="store_true", help="Use --robust")
    ap.add_argument("--save_synthseg_resample", action="store_true", help="Pass --resample to save 1mm resampled images")

    # Registration
    ap.add_argument("--run_registration", action="store_true", help="If set, run SyN and jacobian stats")
    ap.add_argument("--jac_abs_thresh", type=float, default=0.1)

    args = ap.parse_args()
    os.makedirs(args.out_dir, exist_ok=True)

    df = pd.read_csv(args.manifest_csv)
    if args.fold is not None:
        if args.fold_col not in df.columns:
            raise KeyError(f"fold_col='{args.fold_col}' not in manifest")
        df = df[df[args.fold_col].astype(str) == str(args.fold)].reset_index(drop=True)

    if args.max_cases is not None:
        df = df.head(args.max_cases).copy()

    if len(df) == 0:
        raise RuntimeError("No rows to evaluate after filtering.")

    # wandb init
    run_name = datetime.now().strftime("eval_%Y%m%d_%H%M%S")
    if args.wandb_mode == "disabled":
        wandb.init(mode="disabled")
    else:
        wandb.init(project=args.wandb_project, name=run_name, mode=args.wandb_mode, config=vars(args))

    # Locate required columns (manifest schema)
    required = [
        "subject_id",
        "starting_image_uid",
        "followup_image_uid",
        "pred_followup_path",
        "gt_followup_path_AEspace",
        "starting_img_path",
        "starting_segm_path",
        "gt_followup_segm_path",
        "starting_age_months",
        "followup_age_months",
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise KeyError(f"Manifest missing required columns: {missing}")

    # -----------------------
    # 1) Run SynthSeg on all predictions (batch) if requested
    # -----------------------
    pred_seg_dir = os.path.join(args.out_dir, "pred_synthseg")
    os.makedirs(pred_seg_dir, exist_ok=True)

    pred_segs = []
    if args.run_synthseg:
        img_list = df["pred_followup_path"].tolist()
        out_list = []
        for _, row in df.iterrows():
            tag = f"{row.subject_id}_{row.starting_image_uid}_{row.followup_image_uid}"
            out_list.append(os.path.join(pred_seg_dir, f"{tag}_predT2_synthseg.nii.gz"))
        extra = []
        if args.synthseg_fast:
            extra.append("--fast")
        if args.synthseg_robust:
            extra.append("--robust")
        if args.save_synthseg_resample:
            # write resampled 1mm images next to segs
            res_dir = os.path.join(pred_seg_dir, "resampled")
            os.makedirs(res_dir, exist_ok=True)
            # SynthSeg expects same type as --i, so we give it a textfile path
            # but here we pass a folder path is not possible; simplest: skip saving resampled in batch.
            # We'll still include the flag; SynthSeg will try to interpret it like --i and --o.
            # To avoid weirdness, we won't pass it in batch.
            pass

        synthseg_batch(img_list, out_list, threads=args.synthseg_threads, use_cpu=args.synthseg_cpu, extra_flags=extra)
        pred_segs = out_list
    else:
        # if not running synthseg, still *expect* segs might already exist in pred_seg_dir
        # but we won't crash; dice/vol metrics for pred will be missing.
        pred_segs = [None] * len(df)

    # index them by tag for quick lookup
    predseg_by_tag = {}
    for i, row in df.reset_index(drop=True).iterrows():
        tag = f"{row.subject_id}_{row.starting_image_uid}_{row.followup_image_uid}"
        predseg_by_tag[tag] = pred_segs[i] if i < len(pred_segs) else None

    # -----------------------
    # 2) Evaluate per-case
    # -----------------------
    rows_out = []

    for idx, row in df.iterrows():
        sid = row["subject_id"]
        T1 = row["starting_image_uid"]
        T2 = row["followup_image_uid"]
        tag = f"{sid}_{T1}_{T2}"

        pred_t2 = row["pred_followup_path"]
        gt_t2_ae = row["gt_followup_path_AEspace"]

        # Ensure pred and gt are aligned for intensity metrics + registration
        pred_for_metrics = pred_t2
        if not same_grid(pred_t2, gt_t2_ae):
            pred_rs = os.path.join(args.out_dir, "resampled_pred_to_gt", f"{tag}_pred_on_gt.nii.gz")
            pred_for_metrics = resample_to_ref_ants(pred_t2, gt_t2_ae, pred_rs, interp="Linear")

        # Build T1 in AE space to compare baselineâ†’followup in intensity space
        t1_ae_path = os.path.join(args.out_dir, "t1_ae", f"{tag}_T1_AE.nii.gz")
        os.makedirs(os.path.dirname(t1_ae_path), exist_ok=True)
        # Only recompute if missing
        if not os.path.exists(t1_ae_path):
            data = IMG_PIPE_AE({"img": row["starting_img_path"]})["img"]   # torch [1,D,H,W]
            arr = data.cpu().numpy().astype(np.float32)[0]
            # Save with gt affine so comparisons are consistent
            gt_aff = nib.load(gt_t2_ae).affine
            nib.save(nib.Nifti1Image(arr, gt_aff), t1_ae_path)

        # SSIM metrics
        ssim_pred_gt = ssim_3d_monai(pred_for_metrics, gt_t2_ae, device=args.device)
        ssim_t1_gt = ssim_3d_monai(t1_ae_path, gt_t2_ae, device=args.device)

        rec = {
            "fold": row.get("fold", None),
            "subject_id": sid,
            "starting_image_uid": T1,
            "followup_image_uid": T2,
            "starting_age_months": float(row["starting_age_months"]),
            "followup_age_months": float(row["followup_age_months"]),
            "delta_m": float(row["followup_age_months"] - row["starting_age_months"]),
            "sex": row["sex"],
            "pred_followup_path": pred_t2,
            "gt_followup_path_AEspace": gt_t2_ae,
            "starting_img_path": row["starting_img_path"],
            "starting_segm_path": row["starting_segm_path"],
            "gt_followup_segm_path": row["gt_followup_segm_path"],
            "ssim_predT2_vs_gtT2": ssim_pred_gt,
            "ssim_T1_vs_gtT2": ssim_t1_gt,
        }

        # Registration + Jacobian + displacement
        if args.run_registration:
            reg_dir = os.path.join(args.out_dir, "registration")
            warp_path, jac_path = register_and_jacobian_syn(
                pred_path=pred_for_metrics,
                true_path=gt_t2_ae,
                out_dir=reg_dir,
                tag=tag,
            )
            rec["warp_path"] = warp_path
            rec["jac_log_path"] = jac_path

            # optional mask: use GT seg if you want (resample gt seg to AE grid)
            mask = None
            # If you prefer: build mask from gt seg resampled to AE grid
            # (requires a label seg in the same AE space). Here we keep it None by default.

            rec.update(displacement_stats_from_warp(warp_path, mask=mask))
            rec.update(jacobian_stats(jac_path, mask=mask, thresh_abs_logjac=args.jac_abs_thresh))

        # Dice + volumes
        pred_seg = predseg_by_tag.get(tag, None)
        gt_seg = row["gt_followup_segm_path"]
        t1_seg = row["starting_segm_path"]

        # --------------------
        # Volumes for T1 seg
        # --------------------
        if isinstance(t1_seg, str) and os.path.exists(t1_seg):
            vx1 = voxel_volume_mm3(t1_seg)
            seg1 = load_nii_i32(t1_seg)
            rec["total_vol_t1_mm3"] = float((seg1 != 0).sum() * vx1)

            abs1 = compute_abs_volumes(seg1, vx1)
            rel1 = compute_rel_volumes(seg1)
            for k, v in abs1.items():
                rec[k + "_t1"] = v
            for k, v in rel1.items():
                rec[k + "_t1"] = v
        else:
            rec["t1_seg_status"] = "missing"

        # --------------------
        # Volumes for GT T2 seg
        # --------------------
        if isinstance(gt_seg, str) and os.path.exists(gt_seg):
            vx2 = voxel_volume_mm3(gt_seg)
            seg2 = load_nii_i32(gt_seg)
            rec["total_vol_gtT2_mm3"] = float((seg2 != 0).sum() * vx2)

            abs2 = compute_abs_volumes(seg2, vx2)
            rel2 = compute_rel_volumes(seg2)
            for k, v in abs2.items():
                rec[k + "_gt"] = v
            for k, v in rel2.items():
                rec[k + "_gt"] = v
        else:
            rec["gt_seg_status"] = "missing"
            seg2 = None

        # --------------------
        # Dice + volumes for predicted seg (SynthSeg on pred_T2)
        # --------------------
        if pred_seg is not None and isinstance(pred_seg, str) and os.path.exists(pred_seg) and seg2 is not None:
            pred_seg_use = pred_seg
            if not same_grid(pred_seg, gt_seg):
                pred_seg_rs = os.path.join(args.out_dir, "resampled_predseg_to_gtseg", f"{tag}_predseg_on_gtseg.nii.gz")
                pred_seg_use = resample_to_ref_ants(pred_seg, gt_seg, pred_seg_rs, interp="NearestNeighbor")

            segp = load_nii_i32(pred_seg_use)
            vxp = voxel_volume_mm3(gt_seg)  # after resample, use gt seg vx

            rec["total_vol_predT2_mm3"] = float((segp != 0).sum() * vxp)

            absp = compute_abs_volumes(segp, vxp)
            relp = compute_rel_volumes(segp)
            for k, v in absp.items():
                rec[k + "_pred"] = v
            for k, v in relp.items():
                rec[k + "_pred"] = v

            dice = compute_dice_per_label(segp, seg2)
            # store per region dice
            for lbl, sc in dice.items():
                name = SYNTHSEG_CODEMAP.get(int(lbl), f"label_{int(lbl)}")
                rec[f"dice_{name}"] = float(sc)

            # also macro-average dice (over labels present in GT)
            if len(dice) > 0:
                rec["dice_macro"] = float(np.mean(list(dice.values())))
        else:
            rec["pred_seg_status"] = "missing_or_no_gt"

        rows_out.append(rec)

        if (idx + 1) % 10 == 0:
            print(f"[eval] {idx+1}/{len(df)} done")

    out_csv = os.path.join(args.out_dir, f"eval_metrics_{args.fold if args.fold is not None else 'all'}.csv")
    out_json = os.path.join(args.out_dir, f"eval_metrics_{args.fold if args.fold is not None else 'all'}.json")

    df_out = pd.DataFrame(rows_out)
    df_out.to_csv(out_csv, index=False)

    with open(out_json, "w") as f:
        json.dump({"n": len(rows_out), "out_csv": out_csv}, f, indent=2)

    # -----------------------
    # 3) Simple plots
    # -----------------------
    def _save_hist(col: str, fname: str, title: str):
        if col not in df_out.columns:
            return None
        vals = df_out[col].dropna().values
        if len(vals) == 0:
            return None
        plt.figure()
        plt.hist(vals, bins=30)
        plt.title(title)
        plt.xlabel(col)
        plt.ylabel("count")
        p = os.path.join(args.out_dir, fname)
        plt.tight_layout()
        plt.savefig(p, dpi=160)
        plt.close()
        return p

    p1 = _save_hist("ssim_predT2_vs_gtT2", "hist_ssim_pred_vs_gt.png", "SSIM(pred T2, GT T2)")
    p2 = _save_hist("ssim_T1_vs_gtT2", "hist_ssim_T1_vs_gt.png", "SSIM(T1, GT T2)")
    p3 = _save_hist("dice_macro", "hist_dice_macro.png", "Macro Dice (pred seg vs GT seg)")
    p4 = _save_hist("jac_mean_abs_logjac", "hist_jac_mean_abs.png", "Mean |logJac|") if args.run_registration else None

    # -----------------------
    # 4) W&B logging
    # -----------------------
    table = wandb.Table(dataframe=df_out)
    wandb.log({
        "eval/table": table,
        "eval/n_cases": len(df_out),
    })
    for p in [p1, p2, p3, p4]:
        if p is not None and os.path.exists(p):
            wandb.log({os.path.basename(p): wandb.Image(p)})

    wandb.save(out_csv)
    wandb.save(out_json)
    wandb.finish()

    print("[done] wrote:", out_csv)


if __name__ == "__main__":
    main()
